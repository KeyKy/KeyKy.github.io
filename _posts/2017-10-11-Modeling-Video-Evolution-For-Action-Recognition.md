---
layout: post
title: Modeling Video Evolution For Action Recognition
---

[[pdf](https://www.robots.ox.ac.uk/~vgg/rg/papers/videoDarwin.pdf)][[code](https://bitbucket.org/bfernando/videodarwin)]

## Abstract

æœ¬æ–‡å±•ç¤ºäº†ä¸€ç§èƒ½å¤Ÿæ•è·è§†é¢‘æ—¶åºä¿¡æ¯çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å‡å®šèƒ½æ—¶åºæ€§æ’åºè§†é¢‘å¸§çš„å‡½æ•°(a function capable of ording the frames of a video)ä¹Ÿèƒ½éå¸¸å¥½çš„æ•è·è§†é¢‘ä¸­è§†è§‰ä¸Šçš„æ¼”å˜(evolution of the appearance within the video)ã€‚å› æ­¤æœ¬æ–‡çš„é‡ç‚¹æ˜¯ï¼Œä½œè€…é€šè¿‡ä½¿ç”¨ranking machineå­¦ä¹ è¿™æ ·çš„æ’åºå‡½æ•°å¹¶ä¸”ä½¿ç”¨å…¶å¯¹åº”çš„å‚æ•°ä½œä¸ºä¸€ä¸ªæ–°çš„è§†é¢‘è¡¨å¾ã€‚å¹¶åœ¨é€šç”¨åŠ¨ä½œè¯†åˆ«æ•°æ®é›†Hollywood2ã€HMDB51ã€ç»†é¢—ç²’åº¦çš„åŠ¨ä½œMPII-cooking activitieså’Œå§¿åŠ¿æ•°æ®é›†Chalearnä¸­æœ‰7%-10%çš„æå‡ã€‚åŒæ—¶è¯¥æ–¹æ³•æ˜¯ä¸€ç§å¯¹è§†é¢‘è§†è§‰ç‰¹å¾çš„ç¼–ç ï¼Œç‹¬ç«‹äºç‰¹å¾æå–æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´è§†è§‰ç‰¹å¾æå–æ–¹æ³•è¶Šå¥½ï¼Œç¼–ç åå¯¹è§†é¢‘åŠ¨ä½œè¯†åˆ«æ•ˆæœä¹Ÿèƒ½ç›¸åº”æœ‰æå‡ã€‚

## Introduction

åœ¨è¿‡å»åå¹´åŠ¨ä½œè¯†åˆ«çš„ç ”ç©¶ä¸»è¦æ˜¯åœ¨è®¾è®¡æ—¶é—´ç©ºé—´(spatio-temporal)çš„ç‰¹å¾ï¼Œä¸‹é¢æœ‰å…³çš„å‡ ç¯‡è®ºæ–‡ï¼š

1. from temporal interest points over dense sampling to dense trajectories.
    1. [On space-time interest points]
    2. [Learning realistic human actions from movies]
    3. [Dense trajectories and motion boundary descriptors for action recognition]
2. from gradient-based descriptors to motion-based and motion-compensated ones.
    1. [Better exploiting motion for better action recognition]
3. adoption of powerful encoding schemes, Fisher Vectors.
    1. [Action recognition with improved trajectories]

æåˆ°ä»è§†è§‰ä¸Šå»ºæ¨¡æ—¶åºçš„æ¼”å˜ä¿¡æ¯æ¯”è¾ƒå›°éš¾ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¾ˆå¤šæ–¹æ³•å»ºæ¨¡æ—¶åºä¿¡æ¯ï¼šHMMï¼ŒCRFï¼Œdeep network.

Modeling the video-wide temporal evolution of appearance in videos remains a challenging task, due to the **large variability and complexity of video data**. Actions are performed at **largely varying speeds**. Also the speed of the action often **varies non-linearly** within a single video.

è€Œæœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ—¶åºä¿¡æ¯çš„å»ºæ¨¡æ€è·¯ï¼Œå…¶æœ¬è´¨æ¥æºæ˜¯ï¼š

Nevertheless, it is clear that many actions have a characteristic temporal ordering. More precisely, given all the frames
of the video, we learn how to arrange them in chronological order, based on the content of the frames.

<img src='../images/Modeling-Video-Evolution-For-Action-Recognition/1.png' width='600'>

## Related work

ç•¥...

## Modeling Video-wide temporal evolution (VideoDarwin)

1. Video \\(X = [x_1, x_2, ..., x_n]\\) composed of ğ‘› frames and frame at ğ‘¡ is represented by vector \\(x_t \in R^D\\).
2. Define a vector valued function \\(V\\). The output of the vector valued function \\(v_t\\) is obtained by processing all the frames up to time \\(t\\), \\(x_{1:t}\\). For example, the vector \\(v_t\\) can be obtained by applying the mean operation on all of the frames \\(x_{1:t}\\).
3. Define \\(\Psi(v; u) = u^T \cdot v\\). 
4. Namely, the learning to rank problem optimizes the parameters \\(u\\) of the function \\(\Psi(v; u)\\), such that \\(\forall t_i, t_j , v_{t_i} â‰» v_{t_j} \Leftrightarrow u^T â‹…v_{t_i} > u^T â‹…v_{t_j} \\).

è¿™é‡Œçš„æ€æƒ³æ˜¯æ‰¾åˆ°ä¸€ä¸ªå‘é‡\\(u\\),ä½¿å¾—\\(v_i\\)å’Œ\\(v_j\\)åœ¨è¯¥æ–¹å‘ä¸Šçš„æŠ•å½±ä»ç„¶æ»¡è¶³æ—¶åºæ’åºï¼Œé‚£ä¹ˆè¯¥å‘é‡å°±èƒ½è¡¨å¾æ—¶åºä¸Šçš„æ¼”å˜ï¼Œä¹Ÿèƒ½æŠŠè®¸å¤šå¸§ç”¨ä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚è®ºæ–‡ä¸­ç»™å‡ºäº†å‘é‡\\(u\\)çš„ä¼˜åŒ–æ±‚æ³•ï¼Œæ®è®ºæ–‡æ‰€è¿°æ˜¯ä½¿ç”¨RankSVMï¼Œ

$$\mathop{\arg\min}\limits_{u} \frac{1}{2}\left\|u\right\|^2 + C\sum_{\forall i,j v_{t_i} > v_{t_j}} \epsilon_{ij}$$

$$s.t. u^T \cdot (v_{t_i} âˆ’ v_{t_j} ) \geq 1 âˆ’ \epsilon_{ij}$$

$$\epsilon_{ij} \geq 0$$

æ¡ä»¶1ï¼Œ\\(u^T \cdot (v_{t_i} âˆ’ v_{t_j} ) \geq 1 âˆ’ \epsilon_{ij}\\)ï¼Œå³æ˜¯è¦æ»¡è¶³æ’åºæ¡ä»¶å¤§äºä¸€ä¸ªå•ä½é‡å¹¶ä¸”æœ‰ä¸€ä¸ªæ¾å¼›å› å­ï¼Œå¦‚æœæ¾å¼›å› å­è¿‡å¤§ä¼šæƒ©ç½šä¼˜åŒ–å‡½æ•°ã€‚åœ¨ä½œè€…çš„å¼€æºä»£ç (VideoDarwin.m)ä¸­ä½œè€…æ˜¯é€šè¿‡SVRæ¥è§£å†³æ’åºé—®é¢˜(å› ä¸ºSVRæ¯”RankSVMè¦å¿«ï¼Œå¹¶ä¸”å…·æœ‰ç›¸ä¼¼çš„ç»“æœ)ï¼Œæ—¢ç»™æ¯ä¸€å¸§èµ‹äºˆä¸€ä¸ªlabelï¼Œæ¯”å¦‚ç¬¬ä¸€å¸§çš„labelæ˜¯1ï¼Œç¬¬äºŒå¸§æ˜¯2ï¼Œä¾æ¬¡ç±»æ¨...ç„¶åè®­ç»ƒä¸€ä¸ªSVRå›å½’æ¨¡å‹æ±‚å¾—æƒé‡å‘é‡uã€‚å…¶å®æœ€ç®€å•çš„å°±æ˜¯ç”¨çº¿æ€§å›å½’è¿›è¡Œæ±‚è§£ï¼Œåœ¨è®ºæ–‡ä¸­ä¹Ÿè¡¨ç¤ºè¿™æ ·ä¹Ÿæ˜¯å¯è¡Œçš„(any other linear learning to rank method can be employed to learn VideoDarwin)ã€‚

## Vector valued functions for VideoDarwin

è¿™èŠ‚ä¸»è¦æ˜¯æåŠä¸Šé¢æ²¡æœ‰è§£é‡Šçš„å‘é‡ä»·å€¼å‡½æ•°Vçš„é€‰å–ï¼Œè®ºæ–‡ä¸­æ¢å¯»äº†3ç§å½¢å¼çš„å‘é‡ä»·å€¼å‡½æ•°ï¼š

1. Independent Frame Representation. \\(V(t) = \frac{x_t}{\left\|x_t\right\|}\\).
2. Moving Average (MA). \\(\sum_{t}^{t+T} x_t\\).
3. Time Varying Mean Vectors. \\(m_t = \frac{1}{t} \cdot \sum_{i=1}^{t} x_i, v_t = m_t / \left\|m_t\right\|\\).

ä½œè€…é€šè¿‡å®éªŒè¯æ˜ç¬¬ä¸‰ç§æ–¹å¼æ•ˆæœæœ€å¥½ã€‚

<img src='../images/Modeling-Video-Evolution-For-Action-Recognition/3.png' width='450'>

## Experiments

### VideoDarwin

VideoDarwiné€‰å–çš„ç‰¹å¾ï¼š[HOG, HOF, MBH](http://blog.csdn.net/wzmsltw/article/details/52752587) and TRJ. åœ¨å®éªŒä¸­ä½œè€…è¿˜æåˆ°VideoDarwinå‡ ç§å˜å‹:

1. Forward VideoDarwin (FDVD)ï¼Œå°±æ˜¯å°†å¸§æŒ‰æ—¶é—´\\([x_1,x_2,...,x_n]\\)è¿›è¡Œè®­ç»ƒå¾—åˆ°\\(u_{fow}\\).
2. Reverse & Forward VideoDarwin (RFDVD)ï¼Œå°±æ˜¯æ—¢æŒ‰ä¸Šé¢æ–¹å¼å¾—åˆ°\\(u_{fow}\\)ï¼Œç„¶åå°†å¸§é€†åº\\([x_n,x_{n-1},...,x_1]\\)è¿›è¡Œè®­ç»ƒå¾—åˆ°\\(u_{rev}\\).
3. non-linear forward VideoDarwin (NL-FDVD)ï¼Œå°±æ˜¯å¯¹ç‰¹å¾è¿›è¡Œä¸€ä¸ªéçº¿æ€§æ˜ å°„ç„¶åå†è¿›è¡ŒFDVDè®­ç»ƒã€‚
4. nonlinear reverse & forward VideoDarwin (NL-RFDVD)ï¼Œå°±æ˜¯å¯¹ç‰¹å¾è¿›è¡Œä¸€ä¸ªéçº¿æ€§æ˜ å°„ç„¶åå†è¿›è¡ŒRFDVDè®­ç»ƒã€‚

### å¯¹æ¯”çš„baseline

é€‰æ‹©çš„baselineå¯¹æ¯”æ–¹æ³•æ˜¯**local** å’Œ **TP**ï¼Œ For these baselines, at frame level we apply non-linear feature maps (i.e. power normalization for Fisher vectors and chi-squared kernel maps for bag-of-words based methods)ï¼š

**local**: As a first baseline we use the state-of-the-art trajectory features (i.e. improved trajectories and dense trajectories) and pipelines as [1,2]. As this trajectory based baseline mainly considers local temporal information we refer to this baseline as **local**. 

**TP**: We also compare with temporal pyramids (**TP**), by first splitting the video into two equal size subvideos, then computing a representation for each of them like spatial pyramids [3].

### å¯¹æ¯”ç»“æœ

å¯¹æ¯”çš„ç»“æœå¦‚ä¸‹ï¼Œè¿™é‡Œå°±é€‰äº†HMDB51æ•°æ®é›†çš„ç»“æœå±•ç¤ºï¼Œå‰©ä¸‹çš„æ•°æ®é›†çš„ç±»ä¼¼æ•ˆæœï¼Œè¯¦è§è®ºæ–‡ã€‚

<img src='../images/Modeling-Video-Evolution-For-Action-Recognition/4.png' width='450'>

## Conclusion

ä¸€ç§æ— ç›‘ç£çš„æ—¶åºä¿¡æ¯å»ºæ¨¡çš„æ–¹æ³•ã€‚

## References

[1] H. Wang, A. KlÂ¨aser, C. Schmid, and C.-L. Liu. Dense trajectories and motion boundary descriptors for action recognition. IJCV, 103:60â€“79, 2013. 1, 2, 5, 6, 8

[2] H. Wang and C. Schmid. Action recognition with improved trajectories. In ICCV, 2013. 1, 2, 5, 6, 8

[3] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. 1, 5

## ç›¸å…³å·¥ç¨‹å’Œä»£ç 

1. http://lear.inrialpes.fr/~wang/improved_trajectories
2. https://lear.inrialpes.fr/people/wang/dense_trajectories
